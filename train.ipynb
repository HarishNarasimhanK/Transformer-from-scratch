{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e7ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPT import GPT\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import re\n",
    "from GPTDataset import GPTDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a082ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONTEXT_LEN = 128\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02edb166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    dataset = GPTDataset(\"./data/sample.txt\", CONTEXT_LEN)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    print(f\"Vocab size: {dataset.vocab_size}, Dataset size: {len(dataset)}\")\n",
    "\n",
    "    # Create model\n",
    "    model = GPT(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        context_length=CONTEXT_LEN,\n",
    "        model_dimension=EMBED_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_layers=NUM_LAYERS\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        start_time = time.time_ns()\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(next(model.parameters()).device), y.to(next(model.parameters()).device)\n",
    "            \n",
    "            logits, loss = model(x, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}\")\n",
    "        end_time = time.time_ns()\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}, elapsed time : {(end_time - start_time)//1000}\")\n",
    "        \n",
    "    print(\"Model saved!\")\n",
    "    return model, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75925456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, dataset, prompt=\"The main key\", max_tokens=50):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    words = re.findall(r'\\b\\w+\\b', prompt.lower())\n",
    "    idx = torch.tensor([[dataset.stoi.get(word, 0) for word in words]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            if idx.size(1) > CONTEXT_LEN:\n",
    "                idx = idx[:, CONTEXT_LEN:]\n",
    "            \n",
    "            logits, _ = model(idx)\n",
    "            next_token = torch.multinomial(torch.softmax(logits[:, -1, :], dim=-1), 1)\n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "    \n",
    "    # Convert back to text\n",
    "    generated_words = [dataset.itos[i.item()] for i in idx[0]]\n",
    "    return ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca38aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Vocab size: 272, Dataset size: 526\n",
      "Starting training...\n",
      "Epoch 1, Batch 0, Loss: 5.8297\n",
      "Epoch 1 completed. Average Loss: 4.8647, elapsed time : 1282968\n",
      "Epoch 2, Batch 0, Loss: 4.0838\n",
      "Epoch 2 completed. Average Loss: 3.4063, elapsed time : 814111\n",
      "Epoch 3, Batch 0, Loss: 2.6912\n",
      "Epoch 3 completed. Average Loss: 2.1878, elapsed time : 861473\n",
      "Epoch 4, Batch 0, Loss: 1.6992\n",
      "Epoch 4 completed. Average Loss: 1.4883, elapsed time : 812183\n",
      "Epoch 5, Batch 0, Loss: 1.3006\n",
      "Epoch 5 completed. Average Loss: 1.1126, elapsed time : 816361\n",
      "Epoch 6, Batch 0, Loss: 0.9623\n",
      "Epoch 6 completed. Average Loss: 0.8276, elapsed time : 833980\n",
      "Epoch 7, Batch 0, Loss: 0.6818\n",
      "Epoch 7 completed. Average Loss: 0.5832, elapsed time : 787740\n",
      "Epoch 8, Batch 0, Loss: 0.4719\n",
      "Epoch 8 completed. Average Loss: 0.3779, elapsed time : 795950\n",
      "Epoch 9, Batch 0, Loss: 0.2879\n",
      "Epoch 9 completed. Average Loss: 0.2217, elapsed time : 794041\n",
      "Epoch 10, Batch 0, Loss: 0.1779\n",
      "Epoch 10 completed. Average Loss: 0.1266, elapsed time : 787000\n",
      "Epoch 11, Batch 0, Loss: 0.1031\n",
      "Epoch 11 completed. Average Loss: 0.0817, elapsed time : 817405\n",
      "Epoch 12, Batch 0, Loss: 0.0720\n",
      "Epoch 12 completed. Average Loss: 0.0607, elapsed time : 780171\n",
      "Epoch 13, Batch 0, Loss: 0.0509\n",
      "Epoch 13 completed. Average Loss: 0.0493, elapsed time : 781541\n",
      "Epoch 14, Batch 0, Loss: 0.0367\n",
      "Epoch 14 completed. Average Loss: 0.0427, elapsed time : 770511\n",
      "Epoch 15, Batch 0, Loss: 0.0427\n",
      "Epoch 15 completed. Average Loss: 0.0375, elapsed time : 765797\n",
      "Epoch 16, Batch 0, Loss: 0.0370\n",
      "Epoch 16 completed. Average Loss: 0.0337, elapsed time : 799875\n",
      "Epoch 17, Batch 0, Loss: 0.0296\n",
      "Epoch 17 completed. Average Loss: 0.0312, elapsed time : 789646\n",
      "Epoch 18, Batch 0, Loss: 0.0268\n",
      "Epoch 18 completed. Average Loss: 0.0281, elapsed time : 812343\n",
      "Epoch 19, Batch 0, Loss: 0.0276\n",
      "Epoch 19 completed. Average Loss: 0.0259, elapsed time : 811753\n",
      "Epoch 20, Batch 0, Loss: 0.0271\n",
      "Epoch 20 completed. Average Loss: 0.0249, elapsed time : 793958\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "model, dataset = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d7fb1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating sample text:\n",
      "Generated: 000 cell phones while driving in conclusion drivers should regulate able to work a vehicle while using their cell phone drivers who uses their phones while operating a vehicle and are likely to have an accident then those who don t cell phone operation while driving the ability to stay connected to people we know despite distance was originally brought to fruition by the use of letters this system was found to be rather slow and new pathways were searched for until the invention of\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating sample text:\")\n",
    "sample = generate_text(model, dataset, \"watching cell phones while driving\", 80)\n",
    "print(f\"Generated: {sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3720498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, dataset, filename=\"MyGPT.pth\"):\n",
    "    checkpoint = {\n",
    "        # Model weights\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        \n",
    "        # Model configuration\n",
    "        'model_config': {\n",
    "            'vocab_size': dataset.vocab_size,\n",
    "            'context_length': model.context_length,\n",
    "            'model_dimension': model.token_embeddings.embedding_dim,\n",
    "            'num_heads': NUM_HEADS,  # Use the hyperparameter\n",
    "            'num_layers': NUM_LAYERS  # Use the hyperparameter\n",
    "        },\n",
    "        \n",
    "        # Tokenizer data\n",
    "        'tokenizer_data': {\n",
    "            'stoi': dataset.stoi,\n",
    "            'itos': dataset.itos,\n",
    "            'vocab_size': dataset.vocab_size\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"COMPLETE MODEL SAVED SUCCESSFULLY!\")\n",
    "    print(f\"Saved File Name: {filename}\")\n",
    "    print(f\"Model Info:\")\n",
    "    print(f\" - Vocabulary Size: {dataset.vocab_size}\")\n",
    "    print(f\" - Context Length: {model.context_length}\")\n",
    "    print(f\" - Model Dimension: {model.token_embeddings.embedding_dim}\")\n",
    "    print(f\" - Number of Layers: {NUM_LAYERS}\")\n",
    "    print(f\" - Number of Heads: {NUM_HEADS}\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "# Save the complete model\n",
    "save_model(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
